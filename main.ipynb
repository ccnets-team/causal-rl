{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CAUSAL RL\n",
        "==============\n",
        "DELAWARE INCORPORATION\n",
        "COPYRIGHT (c) 2022. CCNets, Inc. All Rights reserved.\n",
        "Author:\n",
        "PARK, JunHo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Causal RL Model Insights:\n",
        "\n",
        "Explore detailed modeling results, including performance metrics and analyses, to understand the advancements in our models across various environments. \n",
        "\n",
        "These insights highlight algorithmic enhancements and overall progress in reinforcement learning capabilities.\n",
        "\n",
        "For a deep dive into our latest modeling outcomes: https://wandb.ai/causal-rl/causal-rl/\n",
        "\n",
        "This documentation serves as a guide to the significant strides made in the field, especially with GPT models in reinforcement learning.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from utils.setting.env_settings import analyze_env\n",
        "from utils.init import set_seed\n",
        "\n",
        "set_seed()\n",
        "ngpu = 2\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available Environments: \n",
        "==============\n",
        "    OpenAI Gymnasium(MuJoCo):\n",
        "        \"HalfCheetah-v4\"\n",
        "        \"Hopper-v4\"\n",
        "        \"Walker2d-v4\"\n",
        "        \"Ant-v4\"\n",
        "        \"HumanoidStandup-v4\"\n",
        "        \"Humanoid-v4\"\n",
        "        \"InvertedDoublePendulum-v4\"\n",
        "        \"Reacher-v4\"\n",
        "        \"Pusher-v4\"\n",
        "        \n",
        "    Unity MLAgents(download link: https://drive.google.com/drive/folders/1TGSfw7IgfmVZslvmqIDLr5jAneQpsVbb?usp=sharing):\n",
        "        locate the downloaded folder as below:\n",
        "        your_projects/\n",
        "            causal-rl/\n",
        "            unity_environments/\n",
        "        \"3DBallHard\"\n",
        "        \"Worm\"\n",
        "        \"Crawler\"\n",
        "        \"Walker\"\n",
        "        \"Hallway\"\n",
        "        \"PushBlock\"\n",
        "        \"Pyramids\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the specified environment to generate recommended RL parameters\n",
        "# 'env_name': The name of the environment to analyze (e.g., OpenAI Gym environments or ML-Agents environments)\n",
        "rl_params = analyze_env(env_name=\"Walker2d-v4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " For configuration settings, check more details at utils/setting/rl_config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import configuration parameter classes for setting up the RL model\n",
        "from utils.setting.rl_config import TrainingParameters, AlgorithmParameters, NetworkParameters, OptimizationParameters, NormalizationParameters\n",
        "from nn.gpt import GPT\n",
        "\n",
        "# Configure training parameters: adjust batch size, max steps, and buffer size for the RL training process\n",
        "rl_params.training = TrainingParameters(batch_size=64, buffer_size=320000)\n",
        "\n",
        "# Set algorithm parameters: Define sequence length, gamma, and lambda for advantage calculation in the RL algorithm\n",
        "rl_params.algorithm = AlgorithmParameters(gpt_seq_length=16)\n",
        "\n",
        "# Configure network parameters: Specify the architecture details of the neural network used in the RL model (e.g., GPT)\n",
        "rl_params.network = NetworkParameters(num_layers=5, d_model=256, network_type=GPT)\n",
        "\n",
        "# Configure optimization parameters including learning rate, minimum learning rate, and maximum gradient norm for gradient clipping\n",
        "# These settings influence the speed and stability of the learning process\n",
        "rl_params.optimization = OptimizationParameters(lr=1e-4, min_lr=1e-5)\n",
        "\n",
        "# Initialize normalization parameters with default settings\n",
        "# This setup affects how state, reward, and advantage values are normalized, impacting the training dynamics\n",
        "rl_params.normalization = NormalizationParameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the Causal RL class for running training and testing\n",
        "from causal_rl import CausalRL\n",
        "\n",
        "# Initialize the RL model with the specified parameters and options\n",
        "with CausalRL(rl_params, device, use_print=True, use_wandb=False) as causal_rl:\n",
        "    # Train the model with the given parameters\n",
        "    # 'resume_training': Whether to resume from a saved checkpoint\n",
        "    # 'use_eval': Whether to evaluate the model periodically during training\n",
        "    # 'use_graphics': Whether to display graphical representations of training progress\n",
        "    causal_rl.train(resume_training=False, use_eval=False, use_graphics=False)\n",
        "    \n",
        "    # Test the trained model over a specified number of episodes\n",
        "    # 'max_episodes': The number of episodes to run during the test\n",
        "    # 'use_graphics': Whether to display graphical representations of the agent's performance\n",
        "    causal_rl.test(max_episodes=100, use_graphics=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "c16dfff7ba1779372f0feb5f1d498cbfa6bad5ce8e2477d9f53bcebd19f9c321"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
