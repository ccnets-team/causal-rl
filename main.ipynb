{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RL TUNE\n",
        "==============\n",
        "DELAWARE INCORPORATION\n",
        "COPYRIGHT (c) 2022. CCNets, Inc. All Rights reserved.\n",
        "Author:\n",
        "PARK, JunHo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from utils.setting.env_settings import analyze_env\n",
        "from utils.init import set_seed\n",
        "\n",
        "set_seed()\n",
        "ngpu = 2\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available Environments: \n",
        "==============\n",
        "    OpenAI Gymnasium(MuJoCo):\n",
        "        \"HumanoidStandup-v4\"\n",
        "        \"Humanoid-v4\"\n",
        "        \"Ant-v4\"\n",
        "        \"Reacher-v4\"\n",
        "        \"Pusher-v4\"\n",
        "        \n",
        "    Unity MLAgents(download link: https://drive.google.com/drive/folders/1TGSfw7IgfmVZslvmqIDLr5jAneQpsVbb?usp=sharing):\n",
        "        locate the downloaded folder as below:\n",
        "        your_projects/\n",
        "            rl-tune/\n",
        "            unity_environments/\n",
        "        \"3DBallHard\"\n",
        "        \"Worm\"\n",
        "        \"Crawler\"\n",
        "        \"Walker\"\n",
        "        \"Hallway\"\n",
        "        \"PushBlock\"\n",
        "        \"Pyramids\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment Specifications for HalfCheetah-v4\n",
            "\n",
            "num_environments: 1, num_agents: 128, samples_per_step: 128\n",
            "state_size: 17, action_size: 6\n",
            "use_discrete: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env_config, rl_params = analyze_env(env_name = \"HalfCheetah-v4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " If your processor is experiencing difficulties with CPU or GPU processing,\n",
        "\n",
        "     consider using a higher value for replay_ratio, perhaps more than 4, as shown below:\n",
        "\n",
        "     rl_params.training.replay_ratio = [specified higher replay ratio]\n",
        "\n",
        "     consider using a lower value for batch_size, perhaps less than 1024, as shown below:\n",
        "\n",
        "     rl_params.training.batch_size = [specified lower batch size]\n",
        "\n",
        " check more details: utils\\setting\\rl_params.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer Name: causal_rl\n",
            "Training Parameters:\n",
            "batch_size: 512, replay_ratio: 4, train_intervel: 20, training_start_step: 2000, \n",
            "\n",
            "Algorithm Parameters:\n",
            "num_td_steps: 16, discount_factor: 0.99, curiosity_factor: 0.0, use_gae_advantage: False, \n",
            "\n",
            "Network Parameters:\n",
            "critic_network: <class 'nn.gpt.GPT'>, actor_network: <class 'nn.gpt.GPT'>, reverse_env_network: <class 'nn.gpt.GPT'>, num_layer: 5, hidden_size: 192, dropout: 0.0, rev_env_hidden_size_mul: 0.5, \n",
            "\n",
            "Optimization Parameters:\n",
            "beta1: 0.9, lr_gamma: 0.9998, step_size: 1, lr: 0.0003, tau: 0.01, \n",
            "\n",
            "Exploration Parameters:\n",
            "noise_type: none, initial_exploration: 1.0, min_exploration: 0.01, decay_percentage: 0.8, decay_mode: linear, max_steps: 400000, \n",
            "\n",
            "Memory Parameters:\n",
            "buffer_type: standard, buffer_size: 1000000, \n",
            "\n",
            "Normalization Parameters:\n",
            "reward_scale: 0.1, state_normalizer: running_z_standardizer, \n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04050be684444787a8989b5a9759ad3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/400000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100/400000] \tbuffer_size: 10752\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 5.25 sec\n",
            "TrainStepReward: -0.4452 \tTestStepReward: -0.6130\n",
            "[200/400000] \tbuffer_size: 23552\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 4.25 sec\n",
            "TrainStepReward: -0.4394 \tTestStepReward: -0.5254\n",
            "[300/400000] \tbuffer_size: 36352\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 4.36 sec\n",
            "TrainStepReward: -0.4218 \tTestStepReward: -0.3798\n",
            "[400/400000] \tbuffer_size: 49152\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 4.27 sec\n",
            "TrainStepReward: -0.4173 \tTestStepReward: -0.3363\n",
            "[500/400000] \tbuffer_size: 61952\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 4.58 sec\n",
            "TrainStepReward: -0.4208 \tTestStepReward: -0.3581\n",
            "[600/400000] \tbuffer_size: 74752\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 4.55 sec\n",
            "TrainStepReward: -0.4226 \tTestStepReward: -0.3993\n",
            "[700/400000] \tbuffer_size: 87552\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 4.17 sec\n",
            "TrainStepReward: -0.4257 \tTestStepReward: -0.4406\n",
            "[800/400000] \tbuffer_size: 100352\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.85 sec\n",
            "TrainStepReward: -0.4296 \tTestStepReward: -0.4524\n",
            "[900/400000] \tbuffer_size: 113152\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.93 sec\n",
            "TrainStepReward: -0.4333 \tTestStepReward: -0.4706\n",
            "[1000/400000] \tbuffer_size: 125824\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.96 sec\n",
            "TrainStepReward: -0.4371 \tTestStepReward: -0.4910\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1100/400000] \tbuffer_size: 136704\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.98 sec\n",
            "TrainStepReward: -0.4338 \tTestStepReward: -0.4532\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1200/400000] \tbuffer_size: 149504\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.76 sec\n",
            "TrainStepReward: -0.4346 \tTestStepReward: -0.4450\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1300/400000] \tbuffer_size: 162304\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.65 sec\n",
            "TrainStepReward: -0.4326 \tTestStepReward: -0.4259\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1400/400000] \tbuffer_size: 175104\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.58 sec\n",
            "TrainStepReward: -0.4335 \tTestStepReward: -0.3791\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1500/400000] \tbuffer_size: 187904\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.34 sec\n",
            "TrainStepReward: -0.4347 \tTestStepReward: -0.4013\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1600/400000] \tbuffer_size: 200704\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.42 sec\n",
            "TrainStepReward: -0.4340 \tTestStepReward: -0.4108\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1700/400000] \tbuffer_size: 213504\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.43 sec\n",
            "TrainStepReward: -0.4352 \tTestStepReward: -0.4134\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1800/400000] \tbuffer_size: 226304\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.36 sec\n",
            "TrainStepReward: -0.4343 \tTestStepReward: -0.4263\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[1900/400000] \tbuffer_size: 239104\n",
            "Opt-Adam lr: 0.0003\n",
            "Time for steps is 3.41 sec\n",
            "TrainStepReward: -0.4356 \tTestStepReward: -0.4393\n",
            "TrainEpisodeRewards: -437.0966 \tTestEpisodeRewards: -490.9551\n",
            "[2000/400000] \tbuffer_size: 251776\n",
            "Opt-Adam lr: 0.00029994\n",
            "Time for steps is 4.18 sec\n",
            "Values: Estimated: 0.0197\tExpected: -0.6777\tAdvantage: -0.6974\t\n",
            "Losses: Value: 4.1253\tCritic: 0.5781\tActor: -0.1689\tRevenv: 2.0321\t\n",
            "CoopErrors: Critic: 0.5973\tActor: 0.5729\tRevenv: 2.0123\t\n",
            "TransitionCosts: Forward: 0.5851\tReverse: 1.3048\tRecurrent: 1.2926\t\n",
            "TrainStepReward: -0.4376 \tTestStepReward: -0.4378\n",
            "TrainEpisodeRewards: -438.1433 \tTestEpisodeRewards: -384.5711\n",
            "[2100/400000] \tbuffer_size: 262656\n",
            "Opt-Adam lr: 0.0002996401799520072\n",
            "Time for steps is 4.63 sec\n",
            "Values: Estimated: -0.4141\tExpected: -0.7364\tAdvantage: -0.3223\t\n",
            "Losses: Value: 2.6074\tCritic: 0.5860\tActor: -0.0052\tRevenv: 1.8115\t\n",
            "CoopErrors: Critic: 0.5975\tActor: 0.6292\tRevenv: 1.8091\t\n",
            "TransitionCosts: Forward: 0.6133\tReverse: 1.2033\tRecurrent: 1.2192\t\n",
            "TrainStepReward: -0.4351 \tTestStepReward: -0.4122\n",
            "TrainEpisodeRewards: -438.1433 \tTestEpisodeRewards: -384.5711\n",
            "[2200/400000] \tbuffer_size: 275456\n",
            "Opt-Adam lr: 0.00029934065960415834\n",
            "Time for steps is 5.06 sec\n",
            "Values: Estimated: -0.4309\tExpected: -0.7378\tAdvantage: -0.3070\t\n",
            "Losses: Value: 2.2496\tCritic: 0.5659\tActor: -0.0160\tRevenv: 1.6477\t\n",
            "CoopErrors: Critic: 0.5767\tActor: 0.6165\tRevenv: 1.6457\t\n",
            "TransitionCosts: Forward: 0.5966\tReverse: 1.1112\tRecurrent: 1.1311\t\n",
            "TrainStepReward: -0.4371 \tTestStepReward: -0.3833\n",
            "TrainEpisodeRewards: -438.1433 \tTestEpisodeRewards: -384.5711\n",
            "[2300/400000] \tbuffer_size: 288256\n",
            "Opt-Adam lr: 0.0002990414386568732\n",
            "Time for steps is 5.15 sec\n",
            "Values: Estimated: -0.4530\tExpected: -0.7577\tAdvantage: -0.3047\t\n",
            "Losses: Value: 2.0060\tCritic: 0.5455\tActor: -0.0220\tRevenv: 1.5107\t\n",
            "CoopErrors: Critic: 0.5560\tActor: 0.6047\tRevenv: 1.5120\t\n",
            "TransitionCosts: Forward: 0.5804\tReverse: 1.0340\tRecurrent: 1.0584\t\n",
            "TrainStepReward: -0.4380 \tTestStepReward: -0.3805\n",
            "TrainEpisodeRewards: -438.1433 \tTestEpisodeRewards: -384.5711\n",
            "[2400/400000] \tbuffer_size: 301056\n",
            "Opt-Adam lr: 0.0002987425168108709\n",
            "Time for steps is 5.50 sec\n",
            "Values: Estimated: -0.4878\tExpected: -0.7711\tAdvantage: -0.2833\t\n",
            "Losses: Value: 1.8358\tCritic: 0.5273\tActor: -0.0124\tRevenv: 1.4031\t\n",
            "CoopErrors: Critic: 0.5385\tActor: 0.5928\tRevenv: 1.4017\t\n",
            "TransitionCosts: Forward: 0.5657\tReverse: 0.9701\tRecurrent: 0.9973\t\n",
            "TrainStepReward: -0.4373 \tTestStepReward: -0.3855\n",
            "TrainEpisodeRewards: -438.1433 \tTestEpisodeRewards: -384.5711\n",
            "[2500/400000] \tbuffer_size: 313856\n",
            "Opt-Adam lr: 0.0002984438937671698\n",
            "Time for steps is 6.94 sec\n",
            "Values: Estimated: -0.5068\tExpected: -0.7721\tAdvantage: -0.2653\t\n",
            "Losses: Value: 1.6890\tCritic: 0.5099\tActor: -0.0070\tRevenv: 1.3180\t\n",
            "CoopErrors: Critic: 0.5222\tActor: 0.5776\tRevenv: 1.3142\t\n",
            "TransitionCosts: Forward: 0.5499\tReverse: 0.9182\tRecurrent: 0.9459\t\n",
            "TrainStepReward: -0.4393 \tTestStepReward: -0.3894\n",
            "TrainEpisodeRewards: -438.1433 \tTestEpisodeRewards: -384.5711\n"
          ]
        }
      ],
      "source": [
        "from training.rl_trainer import RLTrainer  \n",
        "from rl_tune import RLTune\n",
        "\n",
        "trainer = RLTrainer(rl_params, trainer_name='causal_rl')  \n",
        "with RLTune(env_config, trainer, device, use_graphics = False, use_print = True, use_wandb = False) as rl_tune:\n",
        "    rl_tune.train(on_policy=False, resume_training = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with RLTune(env_config, trainer, device, use_graphics = True, use_print = False) as rl_tune:\n",
        "    rl_tune.test(max_episodes = 100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "metadata": {
      "interpreter": {
        "hash": "a7e81af88087f1f4bdc1f0426df14b24fa2673362c5daa7f7f9146748f40b3b1"
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "c16dfff7ba1779372f0feb5f1d498cbfa6bad5ce8e2477d9f53bcebd19f9c321"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
